---
title: 'STATS 205: Final Project Write-Up'
author: "Brian Liu"
date: "6/14/2019"
bibliography: bibliography.bib
output:
  pdf_document: default
  html_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# 1. Background of the data and why it is interesting or important

The data we are using is the data from WHO suicide statistics from Kaggle. This gives population-based statistics on suicide rate [@szamil_2018].

The reason this data is interesting and important is that suicide is prevalent in many times and places around the world, but many places and times have different suicide rates. When it comes to suicide, there are many potential factors or attributes that may be correlated with an increased risk of suicide, such as:

- a person's sex
- the age group a person belongs to
- the generation a person was born in

The goal is to find significant correlations between these factors and suicide rates: that is, does $x$ factor positively predict suicide rate?

The simple inspiration is suicide prevention: If we can identify the factors that correlate positively with, or predict high suicide rates, then we can target our suicide prevention efforts towards populations with those high-risk factors or attributes.

# 2. Explanation of the method studied and its properties

We will use the statistical techniques of nonparametric bootstrap and parametric bootstrap methods to aid in prediction, with linear regression as well (Kendall coefficient), and use cross-validation to test if, given new data for a population, this population is at risk of suicide. In other words, predict if the suicide rate would be abnormally or significantly high, and then compare the performance between the two methods (nonparametric and parametric).

## Bootstrapping

>In statistics, bootstrapping is any test or metric that relies on random sampling with replacement. Bootstrapping allows assigning measures of accuracy (defined in terms of bias, variance, confidence intervals, prediction error or some other such measure) to sample estimates [@efron_tibshirani_1993; @efron_2003]. This technique allows estimation of the sampling distribution of almost any statistic using random sampling methods. Generally, it falls in the broader class of resampling methods[@from_wolfram_mathworld].
>
>Bootstrapping is the practice of estimating properties of an estimator (such as its variance) by measuring those properties when sampling from an approximating distribution. One standard choice for an approximating distribution is the empirical distribution function of the observed data. In the case where a set of observations can be assumed to be from an independent and identically distributed population, this can be implemented by constructing a number of resamples with replacement, of the observed dataset (and of equal size to the observed dataset).
>
>It may also be used for constructing hypothesis tests. It is often used as an alternative to statistical inference based on the assumption of a parametric model when that assumption is in doubt, or where parametric inference is impossible or requires complicated formulas for the calculation of standard errors.

## Nonparametric vs. Parametric bootstrap
 
>Whereas nonparametric bootstraps make no assumptions about how your observations are distributed, and resample your original sample, parametric bootstraps resample a known distribution function, whose parameters are estimated from your sample.
>These bootstrap estimates are either used to attach confidence limits nonparametrically - or a second parametric model is fitted using parameters estimated from the distribution of the bootstrap estimates, from which confidence limits are obtained analytically.
>The advantages and disadvantages of this approach, compared to nonparametric bootstrapping, can be summarised as follows.
>
>In the nonparametric bootstrap, samples are drawn from a discrete set of $n$ observations. This can be a serious disadvantage in small sample sizes because spurious fine structure in the original sample, but absent from the population sampled, may be faithfully reproduced in the simulated data.
>Another concern is that because small samples have only a few values, covering a restricted range, nonparametric bootstrap samples underestimate the amount of variation in the population you originally sampled. As a result, statisticians generally see samples of 10 or less as too small for reliable nonparametric bootstrapping.
>
>Small samples convey little reliable information about the higher moments of their population distribution function - in which case, a relatively simple function may be adequate.
>
>Although parametric bootstrapping provides more power than the nonparametric bootstrap, it does so on the basis of an inherently arbitrary choice of model. Whilst the cumulative distribution of even quite small samples deviate little from that of their population, it can be far from easy to select the most appropriate mathematical function a priori.
>Maximum likelihood estimators are commonly used for parametric bootstrapping despite the fact that this criterion is nearly always based upon their large sample behaviour.
>
>Choosing an appropriate parametric error structure for a statistic based upon small samples can be awkward to justify. Bootstrap t statistics present an additional problem, partly because of problems in estimating standard errors analytically, partly because of difficulties in working out a suitable number of degrees of freedom for your pivot's (presumed, but often large-sample-based) distribution.
>
>So although parametric bootstrapping can be relatively straightforward to perform, and may be used to construct confidence intervals for the sample median of small samples, the bootstrap and estimator distribution functions are often very different. In addition, confidence limits may enclose invalid parameter values, and the coverage error is no better than nonparametric intervals.
>
>Confusingly, whilst the parametric bootstrap is sometimes described as a basic bootstrap, resampling residuals is sometimes referred to as being 'semi parametric' - which is also used to describe test-inversion and smoothed sample bootstraps. Resampling residuals is most popularly used to obtain bootstrap confidence intervals for regression coefficients, for example in nonparametric regression. [@parametric_or_non-parametric_bootstrap]

## Linear regression - Kendall rank correlation coefficient

>In statistics, the Kendall rank correlation coefficient, commonly referred to as Kendall's tau coefficient (after the Greek letter $\tau$), is a statistic used to measure the ordinal association between two measured quantities. A tau test is a non-parametric hypothesis test for statistical dependence based on the tau coefficient.
>
>It is a measure of rank correlation: the similarity of the orderings of the data when ranked by each of the quantities. It is named after Maurice Kendall, who developed it in 1938,[@kendall_1938] though Gustav Fechner had proposed a similar measure in the context of time series in 1897.[@measures_of_association]
>
>Intuitively, the Kendall correlation between two variables will be high when observations have a similar (or identical for a correlation of 1) rank (i.e. relative position label of the observations within the variable: 1st, 2nd, 3rd, etc.) between the two variables, and low when observations have a dissimilar (or fully different for a correlation of -1) rank between the two variables.
>
>Both Kendall's $\tau$ and Spearman's $\rho$ can be formulated as special cases of a more general correlation coefficient.

## Cross validation

>Cross-validation, sometimes called rotation estimation,[@geisser_1993][@acm_digital_library][@devijver_kittler_1982] or out-of-sample testing is any of various similar model validation techniques for assessing how the results of a statistical analysis will generalize to an independent data set. It is mainly used in settings where the goal is prediction, and one wants to estimate how accurately a predictive model will perform in practice. In a prediction problem, a model is usually given a dataset of known data on which training is run (training dataset), and a dataset of unknown data (or first seen data) against which the model is tested (called the validation dataset or testing set).[4][5] The goal of cross-validation is to test the model's ability to predict new data that was not used in estimating it, in order to flag problems like overfitting or selection bias[6] and to give an insight on how the model will generalize to an independent dataset (i.e., an unknown dataset, for instance from a real problem).
>
>One round of cross-validation involves partitioning a sample of data into complementary subsets, performing the analysis on one subset (called the training set), and validating the analysis on the other subset (called the validation set or testing set). To reduce variability, in most methods multiple rounds of cross-validation are performed using different partitions, and the validation results are combined (e.g. averaged) over the rounds to give an estimate of the model's predictive performance.
>
>In summary, cross-validation combines (averages) measures of fitness in prediction to derive a more accurate estimate of model prediction performance.[7]

# 3. Data analysis or simulation study

We will use the crude rate of suicide per 100,000 people.

>This analysis provides information on age-standardized rates...

```{r}
who_suicide_statistics_df <- read.csv("who_suicide_statistics.csv")
head(who_suicide_statistics_df)
colnames(who_suicide_statistics_df)
```

>Filter and save countries with missing suicide rate.

<!-- install.packages("devtools", dependencies=TRUE, repos='http://cran.us.r-project.org') -->

```{r}
library(tidyverse)
filtered_suicide_df <- drop_na(who_suicide_statistics_df, "suicides_no")
head(filtered_suicide_df)
```

>After filtering countries with missing suicide rate, take a random sample of 100 countries and make sure each continent has approximately equal countries.

Filter countries by continent:


```{r}
library(countrycode)
filtered_suicide_df$continent <- countrycode(sourcevar = filtered_suicide_df[, "country"],
                            origin = "country.name",
                            destination = "continent")

head(filtered_suicide_df)
write.csv(filtered_suicide_df, 'filtered_suicide.csv')
```

Let us find out which continents are counted:

```{r}
# Get list of continents
list_of_continents <- unique(filtered_suicide_df$continent); list_of_continents
```

Therefore,

\[
	\frac{100 \textrm{ countries}}{6 \textrm{ continents}} \approx 16 \textrm{ to } 17 \textrm{ countries per continent}
\]

we should randomly sample 17 countries from each continent.


Notably, there are countries that are not on any of the listed continents. Let us see which ones those are:

```{r}
not_in_a_continent = filtered_suicide_df[is.na(filtered_suicide_df$continent),]
write.csv(not_in_a_continent, 'not_in_a_continent.csv')
head(not_in_a_continent)
unique(not_in_a_continent$country)
```

Let us make the choice not to include these countries in the analysis, since there are only two countries.

```{r}
# Take off `NA` from list of continents
list_of_continents <- list_of_continents[-length(list_of_continents)]
list_of_continents
```


We will now create six dataframes, filtered by list of countries for each continent.

<!-- # europe_suicide = filtered_suicide_df[filtered_suicide_df$continent == 'Europe',]
# head(europe_suicide)
# length(europe_suicide$country)
# americas_suicide = filtered_suicide_df[filtered_suicide_df$continent == 'Americas',]
# head(americas_suicide)
# length(americas_suicide$country)
# asia_suicide = filtered_suicide_df[filtered_suicide_df$continent == 'Asia',]
# head(asia_suicide)
# length(asia_suicide$country)
# oceania_suicide = filtered_suicide_df[filtered_suicide_df$continent == 'Oceania',]
# head(oceania_suicide)
# length(oceania_suicide$country) -->
<!-- # africa_suicide = filtered_suicide_df[filtered_suicide_df$continent == 'Africa',]
# head(africa_suicide)
# length(africa_suicide$country) -->

```{r}
# library(rlist)
countries_per_continent <- list()

for (i in seq_along(list_of_continents))
{
	countries_per_continent[[i]] <- filtered_suicide_df[filtered_suicide_df$continent == list_of_continents[i],]
}

length(countries_per_continent)

length(countries_per_continent)

for (i in seq_along(countries_per_continent))
{
	print(head(countries_per_continent[[i]]))
	print(length(countries_per_continent[[i]]))
	cat("\n")
}
```

[This text links to very important information about why a `for` loop doesn't print anything.](https://stackoverflow.com/questions/14987743/head-function-does-not-work-within-for-loop)[^1]

[Link to Pandoc Markdown formatting](https://rmarkdown.rstudio.com/authoring_pandoc_markdown.html%23raw-tex)

[^1]:Basically, `for` loops are functions themselves. R prints out the result of a command automatically, but functions are not inherently a command, and since `for` loops are functions, nothing will be printed. The solution is to have `print(command())` within the `for` loop to get output for your `for` loop. You will never again spend hours trying to find out why a `for` loop doesn't print anything because you're no longer an R newbie.

Randomly sample 17 countries from each continent:

```{r}
list_of_continents
for (i in seq_along(countries_per_continent))
{
	print(list_of_continents[i])
	countries <- unique(countries_per_continent[[i]]$country)
	print(countries)
	print(length(countries))
	cat("\n")
}
```

Since there are only 5 countries in Oceania and 12 countries in Africa, we will use all 5 countries of Oceania and all 12 countries of Africa.

```{r}
samples_of_countries <- list()
num_samples <- 17
for (i in seq_along(countries_per_continent))
{
	countries <- unique(countries_per_continent[[i]]$country)
	current_sample <- list()
	if (length(countries) >= num_samples)
	{
		current_sample <- sample(countries, 17)
	} else {
		current_sample <- sample(countries, length(countries))
	}
	samples_of_countries[[i]] <- current_sample
}
```

Let's see the countries that we will be sampling:

```{r}
total <- 0
for (i in seq_along(samples_of_countries))
{
	print(list_of_continents[i])
	print(samples_of_countries[[i]])
	print(length(samples_of_countries[[i]]))
	total <- total + length(samples_of_countries[[i]])
	cat("\n")
}
total
```


<!-- # Make singular list of countries
# countries_to_test <- list()
# library(rlist)
# for (i in seq_along(samples_of_countries))
# {
# 	current_samples <- samples_of_countries[[i]]
# 	# print(current_samples)
# 	append(x = countries_to_test, values = current_samples)
# }

# length(countries_to_test)

# for (i in seq_along(countries_to_test))
# {
# 	print(countries_to_test[[i]])
# }

# # Not run: 
# x <- list(a=1,b=2,c=3)
# x
# list.append(x,d=4,e=5)
# list.append(x,d=4,f=c(2,3))
# x
# ## End(Not run) -->

Let's filter the original dataframe only to include countries that we have sampled:

```{r}
countries_to_test <- list()
a <- 0
for (i in seq_along(samples_of_countries))
{
	# find out a way to access each country name
	# print each country name
	for (j in seq_along(samples_of_countries[[i]]))
	{
		sample <- samples_of_countries[[i]]
		country_string <- toString(sample[[j]])
		countries_to_test[a] <- country_string
		a <- a + 1
	}
}

length(countries_to_test)
# countries_to_test
```


# 4. Interpretation of the results or discussion

# 5. References

